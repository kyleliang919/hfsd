import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import MllamaForConditionalGeneration, AutoProcessor
from PIL import Image
import requests
import hf_speculative_decoding

target_model_name = "meta-llama/Llama-3.2-90B-Vision-Instruct"
target = MllamaForConditionalGeneration.from_pretrained(target_model_name, torch_dtype=torch.bfloat16, device_map="auto")

drafter_model_name = "meta-llama/Llama-3.2-11B-Vision-Instruct"
drafter = MllamaForConditionalGeneration.from_pretrained(drafter_model_name, torch_dtype=torch.bfloat16, device_map="auto")

# Don't forget to load the tokenizer
processor = AutoProcessor.from_pretrained(target_model_name)


url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg"
image = Image.open(requests.get(url, stream=True).raw)

messages = [
    {"role": "user", "content": [
        {"type": "image"},
        {"type": "text", "text": "If I had to write a haiku for this one, it would be: "}
    ]}
]
input_text = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(image, input_text, return_tensors="pt").to(target.device)
from hf_speculative_decoding import NucleusProcessor
# Parameters
gen_len = 100       # Maximum number of tokens generated (could over pass when using speculative decoding)
gamma = 4           # Number of drafts generated by the drafter model at each step
logits_processor = NucleusProcessor(temperature=.6, top_p=.9) # Nucleus sampling with p=0.9 and T=0.6

# Generate text using the classic auto-regressive decoding (slow)

output = target.generate(**inputs, max_new_tokens=30, use_cache = True)
print("Target decoding:", processor.decode(output[0], skip_special_tokens=True))

output = drafter.generate(**inputs, max_new_tokens=30, use_cache = True)
print("Drafter decoding:", processor.decode(output[0], skip_special_tokens=True))

# Generate text using the speculative decoding (faster)
output_ids_sd, alpha = target.speculative_generate(
                inputs,
                drafter,
                logits_processor=logits_processor,
                gamma=gamma,
                max_new_tokens=gen_len,
                tokenizer = processor.tokenizer,
                use_cache = True
            )
output_sd = processor.tokenizer.decode(output_ids_sd, skip_special_tokens=True)

print("Speculative decoding:", output_sd)
print("Acceptance rate:", alpha) # Number of drafts accepted by the target model divided by the number of drafts generated